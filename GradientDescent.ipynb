{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "conf=SparkConf().setMaster('yarn').setAppName(\"Lab6\")\n",
    "conf.set(\"spark.executor.memory\", \"4g\")\n",
    "conf.set(\"spark.driver.memory\", \"4g\")\n",
    "conf.set(\"spark.cores.max\", \"2\")\n",
    "conf.set(\"spark.yarn.dist.archives\",\"spark.tar.gz#environment\")\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = \"environment/bin/python\"\n",
    "sc=SparkContext(conf=conf)\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare data for modeling\n",
    "+ read file to rdd\n",
    "+ remove header\n",
    "+ split row, convert all field to numeric\n",
    "+ scaled data for better loss surface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawRDD = (sc.textFile(\"hdfs://yarnmaster:9000/data\",8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data + remove header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['\"83\",50000,3650,3,1,2,\"yes\",\"no\",\"no\",\"no\",\"no\",0,\"no\"', '\"97\",53900,8250,3,1,1,\"yes\",\"no\",\"no\",\"no\",\"no\",2,\"no\"', '\"98\",59900,8250,3,1,1,\"yes\",\"no\",\"yes\",\"no\",\"no\",3,\"no\"', '\"103\",125000,4320,3,1,2,\"yes\",\"no\",\"yes\",\"yes\",\"no\",2,\"no\"', '\"104\",132000,3500,4,2,2,\"yes\",\"no\",\"no\",\"yes\",\"no\",2,\"no\"']\n",
      "CPU times: user 22 ms, sys: 2.05 ms, total: 24.1 ms\n",
      "Wall time: 4.16 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# remove header and show the first 05 rows\n",
    "tagsheader = rawRDD.first()\n",
    "header = sc.parallelize([tagsheader])\n",
    "nonHeaderRDD = rawRDD.subtract(header)\n",
    "print(nonHeaderRDD.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split data + convert to numeric\n",
    "+ There is no missing value in ds -> no row to remove\n",
    "+ the price column has 1 value = 1e+05 -> need to convert by hand\n",
    "+ Non numeric column value is only YES or NO -> Convert to 1-0 directly, no need to use sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50000, 3650, 3, 1, 2, 1, 0, 0, 0, 0, 0, 0],\n",
       " [53900, 8250, 3, 1, 1, 1, 0, 0, 0, 0, 2, 0],\n",
       " [59900, 8250, 3, 1, 1, 1, 0, 1, 0, 0, 3, 0],\n",
       " [125000, 4320, 3, 1, 2, 1, 0, 1, 1, 0, 2, 0],\n",
       " [132000, 3500, 4, 2, 2, 1, 0, 0, 1, 0, 2, 0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def splitFunc(x):\n",
    "    res = x.split(',')[1:]\n",
    "    numeric_indexes = [0,1,2,3,4,10]\n",
    "    non_numeric_indexes = [5,6,7,8,9,11]\n",
    "    \n",
    "    for i in numeric_indexes:\n",
    "        if res[i] != \"1e+05\":\n",
    "            res[i] = int(res[i])\n",
    "        else:\n",
    "            res[i] = 100000\n",
    "    for i in non_numeric_indexes:\n",
    "        res[i] = res[i][1:-1]\n",
    "        if res[i] == 'yes':\n",
    "            res[i] = 1\n",
    "        else:\n",
    "            res[i] = 0\n",
    "    return res\n",
    "\n",
    "splittedRDD = nonHeaderRDD.map(splitFunc)\n",
    "data = splittedRDD.collect()\n",
    "splittedRDD.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale data in range 0 - 1\n",
    "#### i want to treat every feature equally -> normalize all features to 0-1\n",
    "#### i want to keep price the same (no scaled) to make big penalty to error\n",
    "+ max size = 10000 (estimate)\n",
    "+ max #of bedrooms, bathrooms, stories = 3 (estimate)\n",
    "\n",
    "=> Divide each column by max value to scaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50000,\n",
       "  0.365,\n",
       "  1.0,\n",
       "  0.3333333333333333,\n",
       "  0.6666666666666666,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0]]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scaledFunc(x):\n",
    "    x[0] = x[0]\n",
    "    x[1] = x[1]/10000\n",
    "    x[2] = x[2]/3\n",
    "    x[3] = x[3]/3\n",
    "    x[4] = x[4]/3\n",
    "    return x\n",
    "\n",
    "scaledRDD = splittedRDD.map(scaledFunc)\n",
    "scaledRDD.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert data to labeledPoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsePoint(x):\n",
    "    return LabeledPoint(x[0], np.array(x[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[LabeledPoint(50000.0, [0.365,1.0,0.3333333333333333,0.6666666666666666,1.0,0.0,0.0,0.0,0.0,0.0,0.0])]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labeledRDD = scaledRDD.map(parsePoint)\n",
    "labeledRDD.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split 80-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [.8, .2]\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainRDD, testRDD = labeledRDD.randomSplit(weights, seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cache train rdd for later use (because we need to use it frequently)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[16] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainRDD.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "nTrain = trainRDD.count()\n",
    "nTest = testRDD.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "446 100\n"
     ]
    }
   ],
   "source": [
    "print(nTrain, nTest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TRAIN THE MODEL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to calculate MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def squaredError(label, prediction):\n",
    "    \"\"\"Calculates the the squared error for a single prediction.\n",
    "\n",
    "    Args:\n",
    "        label (float): The correct value for this observation.\n",
    "        prediction (float): The predicted value for this observation.\n",
    "\n",
    "    Returns:\n",
    "        float: The difference between the `label` and `prediction` squared.\n",
    "    \"\"\"\n",
    "    return (label-prediction)**2\n",
    "\n",
    "def calcRMSE(labelsAndPreds):\n",
    "    \"\"\"Calculates the root mean squared error for an `RDD` of (label, prediction) tuples.\n",
    "\n",
    "    Args:\n",
    "        labelsAndPred (RDD of (float, float)): An `RDD` consisting of (label, prediction) tuples.\n",
    "\n",
    "    Returns:\n",
    "        float: The square root of the mean of the squared errors.\n",
    "    \"\"\"\n",
    "    return np.sqrt(labelsAndPreds.map(lambda x: squaredError(x[0], x[1])).sum()/labelsAndPreds.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to do the inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLabeledPrediction(weights, observation):\n",
    "    \"\"\"Calculates predictions and returns a (label, prediction) tuple.\n",
    "\n",
    "    Note:\n",
    "        The labels should remain unchanged as we'll use this information to calculate prediction\n",
    "        error later.\n",
    "\n",
    "    Args:\n",
    "        weights (np.ndarray): An array with one weight for each features in `trainData`.\n",
    "        observation (LabeledPoint): A `LabeledPoint` that contain the correct label and the\n",
    "            features for the data point.\n",
    "\n",
    "    Returns:\n",
    "        tuple: A (label, prediction) tuple.\n",
    "    \"\"\"\n",
    "    \n",
    "    return (observation.label, weights.dot(observation.features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function to calculate the gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientSummand(weights, lp):\n",
    "    \"\"\"Calculates the gradient summand for a given weight and `LabeledPoint`.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        weights (DenseVector): An array of model weights (betas).\n",
    "        lp (LabeledPoint): The `LabeledPoint` for a single observation.\n",
    "\n",
    "    Returns:\n",
    "        DenseVector: An array of values the same length as `weights`.  The gradient summand.\n",
    "    \"\"\"\n",
    "    return (weights.dot(lp.features) - lp.label) * lp.features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train loop function\n",
    "+ I experimented with different alpha value and found 0.5 is a good ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linregGradientDescent(trainData, numIters, alpha=0.5):\n",
    "    \"\"\"Calculates the weights and error for a linear regression model trained with gradient descent.\n",
    "\n",
    "    Note:\n",
    "        `DenseVector` behaves similarly to a `numpy.ndarray` and they can be used interchangably\n",
    "        within this function.  For example, they both implement the `dot` method.\n",
    "\n",
    "    Args:\n",
    "        trainData (RDD of LabeledPoint): The labeled data for use in training the model.\n",
    "        numIters (int): The number of iterations of gradient descent to perform.\n",
    "\n",
    "    Returns:\n",
    "        (np.ndarray, np.ndarray): A tuple of (weights, training errors).  Weights will be the\n",
    "            final weights (one weight per feature) for the model, and training errors will contain\n",
    "            an error (RMSE) for each iteration of the algorithm.\n",
    "    \"\"\"\n",
    "    # The length of the training data\n",
    "    n = nTrain\n",
    "    # The number of features in the training data\n",
    "    d = 11\n",
    "    w = np.ones(d)*1000\n",
    "    # We will compute and store the training error after each iteration\n",
    "    errorTrain = np.zeros(numIters)\n",
    "    for i in range(numIters):\n",
    "        # Use getLabeledPrediction from (3b) with trainData to obtain an RDD of (label, prediction) tuples. \n",
    "        # Note that the weights all equal 0 for the first iteration, so the predictions will have large errors to start.\n",
    "        labelsAndPredsTrain = trainData.map(lambda x: getLabeledPrediction(w, x))\n",
    "        errorTrain[i] = calcRMSE(labelsAndPredsTrain)\n",
    "\n",
    "        # Calculate the `gradient`.  Make use of the `gradientSummand` function you wrote in (3a).\n",
    "        # Note that `gradient` sould be a `DenseVector` of length `d`.\n",
    "        gradient = trainData.map(lambda x: gradientSummand(w, x)).sum()/ n\n",
    "        # Update the weights\n",
    "        w -= alpha * gradient\n",
    "        \n",
    "    return w, errorTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 527 ms, sys: 104 ms, total: 631 ms\n",
      "Wall time: 53.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "numIters = 30\n",
    "weightsLR0, errorTrainLR0 = linregGradientDescent(trainRDD, numIters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[69142.04074196 60838.25518856 53717.53315813 47615.58336835\n",
      " 42397.14081003 37948.61936465 34172.88944735 30985.45905976\n",
      " 28311.64343748 26084.49998288 24243.40797551 22733.2043569\n",
      " 21503.76940994 20509.92033466 19711.44943075 19073.15696539\n",
      " 18564.77512193 18160.7399029  17839.82068516 17584.64929084\n",
      " 17381.20068253 17218.27203952 17086.99418223 16980.39540761\n",
      " 16893.02622696 16820.64538632 16759.9626746  16708.43162931\n",
      " 16664.08451207 16625.4021711 ]\n"
     ]
    }
   ],
   "source": [
    "print(errorTrainLR0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[18604.06268639 15767.27159834 19503.45243351 18240.86214189\n",
      " 10368.46507724  5955.71681927  5266.15796703  8882.02828151\n",
      " 14653.91068647  6081.02775781  9253.26756527]\n"
     ]
    }
   ],
   "source": [
    "print(weightsLR0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate test set with RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial error:  61475.99608307891\n",
      "Final error:  13901.918535607321\n"
     ]
    }
   ],
   "source": [
    "labelsAndPredsTest = testRDD.map(lambda x: getLabeledPrediction(np.ones(11)*1000, x))\n",
    "print(\"Initial error: \", calcRMSE(labelsAndPredsTest))\n",
    "labelsAndPredsTest = testRDD.map(lambda x: getLabeledPrediction(weightsLR0, x))\n",
    "print(\"Final error: \", calcRMSE(labelsAndPredsTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(43000.0, 51773.98575540007),\n",
       " (50000.0, 58611.08828318928),\n",
       " (74500.0, 69376.76050906078),\n",
       " (87250.0, 68242.25089627532),\n",
       " (46200.0, 62091.25623232394)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labelsAndPredsTest.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
